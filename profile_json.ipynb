{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python385jvsc74a57bd0aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49",
   "display_name": "Python 3.8.5 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Exploring JSON options in Spark"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.types as T\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"demo\") \\\n",
    "    .getOrCreate()\n",
    "spark.conf.set(\"spark.sql.repl.eagerEval.enabled\", True)"
   ]
  },
  {
   "source": [
    "## Comparing auto-inferred schemas vs explicit / manual entry of schema"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "root\n |-- browser_id: string (nullable = true)\n |-- item_skus: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "inferred_df = spark.read.json(\"clickstream.json\")\n",
    "inferred_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-----------------+----------------------------+\n|browser_id       |item_skus                   |\n+-----------------+----------------------------+\n|e0849a8e34f825496|item_1,item_2,item_3        |\n|82f7694c1b1afbb28|[\"item_1\",\"item_2\",\"item_3\"]|\n+-----------------+----------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "inferred_df.show(10, truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "root\n |-- browser_id: string (nullable = true)\n |-- item_skus: array (nullable = true)\n |    |-- element: string (containsNull = true)\n\n"
     ]
    }
   ],
   "source": [
    "manual_df = spark.read.json(\"clickstream.json\", \n",
    "    T.StructType([\n",
    "        T.StructField(\"browser_id\", T.StringType()), \n",
    "        T.StructField(\"item_skus\", T.ArrayType(T.StringType()))\n",
    "    ]\n",
    "))\n",
    "\n",
    "manual_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-----------------+------------------------+\n|browser_id       |item_skus               |\n+-----------------+------------------------+\n|e0849a8e34f825496|null                    |\n|82f7694c1b1afbb28|[item_1, item_2, item_3]|\n+-----------------+------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "manual_df.show(10, truncate = False)"
   ]
  },
  {
   "source": [
    "## UDF For Profiling JSON"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "import json\n",
    "\n",
    "import pyspark.sql.types as T\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "def append_type(out, prefix, type):\n",
    "    prev_type_list = out.get(prefix)\n",
    "    if prev_type_list:\n",
    "        if type not in prev_type_list:\n",
    "            prev_type_list.append(type)\n",
    "    else:\n",
    "        out[prefix] = [type]\n",
    "\n",
    "def create_types(x, prefix, out):\n",
    "    if type(x) is dict:\n",
    "        for a in x:\n",
    "            descended_prefix = prefix + \"{}.\" + str(a)\n",
    "            create_types(x[a], descended_prefix, out)\n",
    "    elif type(x) is list:\n",
    "        for a in x:\n",
    "            create_types(a, prefix + \"[]\", out)\n",
    "    elif type(x) is str:\n",
    "        append_type(out, prefix, \"str\")\n",
    "    elif type(x) is int:\n",
    "        append_type(out, prefix, \"int\")\n",
    "    elif type(x) is float:\n",
    "        append_type(out, prefix, \"float\")\n",
    "    elif x is True or x is False:\n",
    "        append_type(out, prefix, \"bool\")\n",
    "    elif x is None:\n",
    "        append_type(out, prefix, \"null\")\n",
    "    else:\n",
    "        append_type(out, prefix, \"unknown type: \" + str(type(x)))\n",
    "    return out\n",
    "\n",
    "@F.udf(returnType=T.ArrayType(\n",
    "    T.StructType([\n",
    "        T.StructField(\"field_path\", T.StringType()), \n",
    "        T.StructField(\"field_type\", T.StringType())\n",
    "    ])))\n",
    "def profile_json(obj_str, prefix):\n",
    "    if obj_str:\n",
    "        type_map = create_types(json.loads(obj_str), prefix, {})\n",
    "        rtn = []\n",
    "        for field_path in type_map:\n",
    "            for field_type in type_map[field_path]:\n",
    "                rtn.append((field_path, field_type))\n",
    "        return rtn\n",
    "\n",
    "def json_profile(df, json_col):\n",
    "    return df \\\n",
    "        .withColumn(\"profile\", F.explode(profile_json(json_col, F.lit(\"json_data\")))) \\\n",
    "        .select(\n",
    "            F.col(\"profile.field_path\").alias(\"field_path\"),\n",
    "            F.col(\"profile.field_type\").alias(\"field_type\")\n",
    "        ) \\\n",
    "        .groupBy(\"field_path\") \\\n",
    "        .agg(\n",
    "            F.collect_set(\"field_type\").alias(\"field_types\"),\n",
    "            F.count(\"*\").alias(\"count\"),\n",
    "        )\n",
    "        "
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-----------------------+-----------+-----+\n|field_path             |field_types|count|\n+-----------------------+-----------+-----+\n|json_data{}.browser_id |[str]      |2    |\n|json_data{}.item_skus  |[str]      |1    |\n|json_data{}.item_skus[]|[str]      |1    |\n+-----------------------+-----------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "plain_line_df = spark.read.text(\"clickstream.json\")\n",
    "json_profile(plain_line_df, plain_line_df.value).show(10, truncate = False)"
   ]
  },
  {
   "source": [
    "## A slightly more comprehensive example"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_profile_w_ts(df, json_col):\n",
    "    return df \\\n",
    "        .withColumn(\"profile\", F.explode(profile_json(json_col, F.lit(\"json_data\")))) \\\n",
    "        .select(\n",
    "            F.col(\"profile.field_path\").alias(\"field_path\"),\n",
    "            F.col(\"profile.field_type\").alias(\"field_type\"),\n",
    "            F.get_json_object(\"value\", \"$.event_ts\").cast(T.TimestampType()).alias(\"event_ts\")\n",
    "        ) \\\n",
    "        .groupBy(\"field_path\", \"field_type\") \\\n",
    "        .agg(\n",
    "            F.min(\"event_ts\").alias(\"first_seen\"), \n",
    "            F.max(\"event_ts\").alias(\"last_seen\"),\n",
    "            F.count(\"*\").alias(\"count\"),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-------------------------------+----------+-------------------+-------------------+-----+\n|field_path                     |field_type|first_seen         |last_seen          |count|\n+-------------------------------+----------+-------------------+-------------------+-----+\n|json_data{}.device{}.browser_id|int       |2020-01-01 08:30:39|2020-06-01 20:46:04|317  |\n|json_data{}.device{}.browser_id|str       |2020-06-02 07:45:29|2021-05-26 09:15:58|683  |\n|json_data{}.device{}.ip        |str       |2020-01-01 08:30:39|2020-12-20 22:22:58|700  |\n|json_data{}.device{}.user_agent|str       |2020-01-01 08:30:39|2021-05-26 09:15:58|1000 |\n|json_data{}.event              |str       |2020-01-01 08:30:39|2021-05-26 09:15:58|1000 |\n|json_data{}.event_ts           |str       |2020-01-01 08:30:39|2021-05-26 09:15:58|1000 |\n+-------------------------------+----------+-------------------+-------------------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "plain_line_df = spark.read.text(\"clickstream_full.json\")\n",
    "json_profile_w_ts(plain_line_df, plain_line_df.value).orderBy(\"field_path\").show(10, truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}